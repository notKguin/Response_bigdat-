{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4f1465-c8bd-40d9-b723-295a3de834f6",
   "metadata": {},
   "source": [
    "# Hadoop: Map-Reduce "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb892ef-cb04-4eb1-b1d2-a23063f94cb2",
   "metadata": {},
   "source": [
    "ПРИМЕЧАНИЕ: для этого блокнота вам следует запустить сервер с Hadoop (с YARN) и средой Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce7b96-59a4-4333-ac1e-8e4c69c9131b",
   "metadata": {},
   "source": [
    "[Библиотека программного обеспечения Apache Hadoop](https://hadoop.apache.org/) — это фреймворк, который позволяет выполнять распределенную обработку больших наборов данных в кластерах компьютеров с использованием простых моделей программирования. Он разработан для масштабирования от отдельных серверов до тысяч машин, каждая из которых предлагает локальные вычисления и хранение. Вместо того, чтобы полагаться на оборудование для обеспечения высокой доступности, сама библиотека разработана для обнаружения и обработки сбоев на уровне приложений, тем самым предоставляя высокодоступный сервис поверх кластера компьютеров, каждый из которых может быть подвержен сбоям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a6736-fa64-4dec-a30b-4b8eed443419",
   "metadata": {},
   "source": [
    "Текущая установка включает следующие модули из экосистемы Hadoop:\n",
    "\n",
    "- __Hadoop Common:__ Общие утилиты, которые поддерживают другие модули Hadoop.\n",
    "- __Hadoop Distributed File System (HDFS™):__ Распределенная файловая система, которая обеспечивает высокопроизводительный доступ к данным приложений.\n",
    "- __Hadoop YARN:__ Фреймворк для планирования заданий и управления ресурсами кластера.\n",
    "- __Hadoop MapReduce:__ Система на основе YARN для параллельной обработки больших наборов данных.\n",
    "- __Spark™:__ Быстрый и универсальный вычислительный движок для данных Hadoop. Spark предоставляет простую и выразительную модель программирования, которая поддерживает широкий спектр приложений, включая ETL, машинное обучение, потоковую обработку и вычисление графов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af068185-8cea-4d37-b731-6c47ad25483a",
   "metadata": {},
   "source": [
    "Псевдораспределенный режим (кластер с одним узлом)\n",
    "\n",
    "Hadoop также может быть запущен на одном узле в [псевдораспределенном режиме](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation), где каждый демон Hadoop работает в отдельном процессе Java.\n",
    "\n",
    "В псевдораспределенном режиме мы также используем только один узел, но главное, что кластер моделируется, что означает, что все процессы внутри кластера будут работать независимо друг от друга. Все демоны, которые являются Namenode, Datanode, Secondary Name node, Resource Manager, Node Manager и т. д., будут работать как отдельный процесс на отдельной JVM (виртуальной машине Java) или, можно сказать, работать на разных процессах Java, поэтому он называется псевдораспределенным."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da3b81-3f3d-4add-8ef2-9a7a768c02e6",
   "metadata": {},
   "source": [
    "<b>ОЧЕНЬ ВАЖНОЕ ПРИМЕЧАНИЕ: экземпляр Hadoop, установленный в среде «Hadoop (с YARN) и Spark», был разработан только для образовательных целей и НЕ СОХРАНЯЕТ ДАННЫЕ после остановки сервера. Вы можете создавать или удалять файлы в файловой системе HDFS, записывать данные во время сеанса, но при следующем запуске сервера Jupyter будет чистая файловая система без данных.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9f3374-55b8-4a7c-961f-e42ef30b194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import utils as pu\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccf53bf-f380-455f-996a-f326ec31ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "YARN_PORT = 8088\n",
    "\n",
    "# директория по умолчанию для пользователя `jovyan`\n",
    "WORK_DIR = '/jovyan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458ac703-b481-4fbf-bdae-dfdcf65dd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_dirs(path, filter_str=''):\n",
    "    \"\"\"\n",
    "    Возвращает файлы по указанному пути в виде списка. \n",
    "    Имена файлов можно фильтровать по параметру `filter_str`,\n",
    "    например, `filter_str='csv'` отобразит только файлы `csv`.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-ls', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    dirs = out.decode('utf-8').split('\\n')\n",
    "    dirs = list(filter(lambda x: filter_str in x, dirs))\n",
    "    dirs = list(map(lambda x: x.split(' ')[-1], dirs))\n",
    "    return dirs\n",
    "\n",
    "def file_content(path):\n",
    "    \"\"\"\n",
    "    Возвращает содержимое файла.\n",
    "    Аналогично команде `cat`.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-cat', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    return out.decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79854c2-cbfd-4ab3-8050-65195dfa3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - hadoop supergroup           0 2025-06-30 04:37 /hbase\n",
      "drwxr-xr-x   - jovyan hadoopusers          0 2025-06-30 04:37 /jovyan\n",
      "drwxrwxrwx   - hadoop supergroup           0 2025-06-30 04:37 /tmp\n",
      "drwxr-xr-x   - jovyan hadoopusers          0 2025-06-30 04:37 /user\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbe385-501d-43e7-bd51-024a5f61082d",
   "metadata": {},
   "source": [
    "Посмотрим, что лежит в рабочей директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2832c29-fad1-40d7-8fca-1a6efcafca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7185b9d-a4ab-4114-bc7d-1d9b275a484b",
   "metadata": {},
   "source": [
    "Здесь пусто. \n",
    "\n",
    "\n",
    "Скопируем файл с логами студентов на образовательном портале. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b408d4-93bf-47f5-b629-3b288b5acf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put aggrigation_logs_per_week.csv {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fcd1316-8261-4eee-9f4e-744bcc7fcea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 jovyan hadoopusers   39030677 2025-06-30 04:39 /jovyan/aggrigation_logs_per_week.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78547df8-e56b-494c-83d6-1c167f5ae864",
   "metadata": {},
   "source": [
    "Вызовем фанкцию вывода списка файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "879659cb-18d3-4d2f-82e6-e5903fac613f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/jovyan/aggrigation_logs_per_week.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_dirs(WORK_DIR, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e08474-3c36-4a52-a0cb-196a704c550b",
   "metadata": {},
   "source": [
    "Выведем первые 100 символов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df33ebd7-5dc4-4529-b930-bf562bfa021c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'courseid,userid,num_week,s_all,s_all_avg,s_course_viewed,s_course_viewed_avg,s_q_attempt_viewed,s_q_'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content ('/jovyan/aggrigation_logs_per_week.csv')[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2b4e5-ed45-4cc3-8f35-ce2b7384c41d",
   "metadata": {},
   "source": [
    "# Команда hdfs fsck\n",
    "\n",
    "hdfs fsck — это утилита для проверки целостности файлов и блоков в HDFS. Она позволяет выявить проблемы, такие как отсутствующие блоки, поврежденные данные или другие несоответствия.\n",
    "\n",
    "Аргументы: \n",
    "- files \n",
    "вывести информацию о файле, включая его размер, количество блоков и другие метаданные.\n",
    "\n",
    "- blocks\n",
    "добавляет информацию о блоках, из которых состоит файл. В HDFS файлы разбиваются на блоки (обычно размером 128 МБ или 256 МБ), которые распределяются по узлам кластера. Этот флаг покажет, сколько блоков занимает файл, их размеры и статус.\n",
    "\n",
    "- racks\n",
    "добавляет информацию о том, на каких стойках (racks) расположены блоки файла. В HDFS стойки используются для обеспечения отказоустойчивости и оптимизации производительности. Этот флаг покажет, как блоки распределены по стойкам в кластере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13688ee-ad48-4aab-9b62-43838a718ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://0.0.0.0:9870/fsck?ugi=jovyan&files=1&blocks=1&racks=1&path=%2Fjovyan%2Faggrigation_logs_per_week.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSCK started by jovyan (auth:SIMPLE) from /10.112.133.96 for path /jovyan/aggrigation_logs_per_week.csv at Mon Jun 30 04:39:34 UTC 2025\n",
      "\n",
      "/jovyan/aggrigation_logs_per_week.csv 39030677 bytes, replicated: replication=1, 1 block(s):  OK\n",
      "0. BP-743194254-10.112.133.96-1751258213750:blk_1073741837_1013 len=39030677 Live_repl=1  [/default-rack/10.112.133.96:9866]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t1\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t39030677 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 39030677 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t1\n",
      " Average block replication:\t1.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Mon Jun 30 04:39:34 UTC 2025 in 7 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/jovyan/aggrigation_logs_per_week.csv' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs fsck /jovyan/aggrigation_logs_per_week.csv -files -blocks -racks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676111c8-ba26-4b06-8951-ab4ae6737e4d",
   "metadata": {},
   "source": [
    "# Пояснения к выводу\n",
    "Connecting to namenode via http://0.0.0.0:9870/fsck?ugi=jovyan&files=1&blocks=1&racks=1&path=%2Fjovyan%2Faggrigation_logs_per_week.csv\n",
    "FSCK started by jovyan (auth:SIMPLE) from /10.112.129.148 for path /jovyan/aggrigation_logs_per_week.csv at Mon Jan 27 11:39:52 UTC 2025\n",
    "\n",
    "/jovyan/aggrigation_logs_per_week.csv 39030677 bytes, replicated: replication=1, 1 block(s):  OK\n",
    "\n",
    "#### Размер файла: 39 030 677 байт (~37 МБ).\n",
    "#### Репликация: replication=1 (файл хранится только в одном экземпляре).\n",
    "#### Число блоков: 1 (весь файл помещён в один блок).\n",
    "#### Статус: OK (файл здоров).\n",
    "\n",
    "<i> 0. BP-433375171-10.112.129.148-1737977684764:blk_1073741837_1013 len=39030677 Live_repl=1  [/default-rack/10.112.129.148:9866]  </i>\n",
    "#### Информация о блоке:\n",
    "#### Идентификатор блока: blk_1073741837_1013.\n",
    "#### Длина блока: len=39030677 (совпадает с размером файла).\n",
    "#### Количество активных реплик: Live_repl=1 (блок доступен только на одном узле).\n",
    "#### Расположение блока: /default-rack/10.112.129.148:9866 (узел на rack'е default-rack).\n",
    "\n",
    "Status: HEALTHY\n",
    " Number of data-nodes:\t1\n",
    " Number of racks:\t\t1\n",
    " Total dirs:\t\t\t0\n",
    " Total symlinks:\t\t0\n",
    "\n",
    "#### HDFS работает с одним узлом хранения данных.\n",
    "#### Rack'ов (групп узлов) тоже один.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cddd2-bfd2-477f-829c-4bb8776660a1",
   "metadata": {},
   "source": [
    "# Команда hdfs dfs -setrep\n",
    "\n",
    "команда HDFS, которая изменяет фактор репликации (количество копий) для указанного файла или директории.\n",
    "\n",
    "Пример использования:\n",
    "```\n",
    "%%bash\n",
    "hdfs dfs -setrep -w 2 /jovyan/aggrigation_logs_per_week.csv\n",
    "```\n",
    "Аргументы команды:\n",
    "- -w\n",
    "Этот флаг указывает, что команда должна ожидать завершения процесса репликации. Без этого флага команда просто запустит процесс репликации в фоновом режиме.\n",
    "\n",
    "- 2\n",
    "Это новое значение фактора репликации. В данном случае файл будет иметь 2 копии (реплики) в HDFS.\n",
    "\n",
    "- /jovyan/aggrigation_logs_per_week.csv\n",
    "Это путь к файлу в HDFS, для которого изменяется фактор репликации.\n",
    "\n",
    "\n",
    "В нашей среде данная команда не сработает, поскольку у нас в наличие только одна DataNode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff405c7-f8dc-4051-acb7-74a1bb44f1b9",
   "metadata": {},
   "source": [
    "## Map-reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bbcd5-a50e-4a5d-8b94-828d419684d2",
   "metadata": {},
   "source": [
    "Hadoop MapReduce — это программная среда для простого написания приложений, которые обрабатывают огромные объемы данных (многотерабайтные наборы данных) параллельно на больших кластерах (тысячи узлов) стандартного оборудования надежным и отказоустойчивым способом.\n",
    "\n",
    "Задание MapReduce обычно разбивает входной набор данных на независимые фрагменты, которые обрабатываются задачами <b>map</b> полностью параллельно. Среда сортирует выходные данные карт, которые затем вводятся в задачи <b>reduce</b>. Обычно и входные, и выходные данные задания хранятся в файловой системе. Среда занимается планированием задач, их мониторингом и повторно выполняет невыполненные задачи.\n",
    "\n",
    "Среда MapReduce работает исключительно с парами <ключ, значение>, то есть среда рассматривает входные данные задания как набор пар <ключ, значение> и создает набор пар <ключ, значение> в качестве выходных данных задания, предположительно разных типов.\n",
    "\n",
    "Типы входных и выходных данных задания MapReduce:\n",
    "```\n",
    "(вход) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (выход)\n",
    "```\n",
    "\n",
    "Демонстрация для фреймворка MapReduce предназначена для хорошо известной задачи подсчета слов:\n",
    "\n",
    "![MapReduce](https://www.todaysoftmag.com/images/articles/tsm33/large/a11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d40fe3-6981-4e1c-9691-03a382be25f5",
   "metadata": {},
   "source": [
    "# Задание 1: Подсчет количества записей для каждого курса (courseid)\n",
    "\n",
    "Цель: Научиться использовать MapReduce для подсчета количества записей для каждого курса.\n",
    "\n",
    "Ход выполнения:\n",
    "Map: Преобразуйте каждую строку в пару ключ-значение, где ключ — это courseid, а значение — 1.\n",
    "\n",
    "Reduce: Суммируйте значения для каждого ключа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ace8ca5c-864a-4eb6-9c57-616d3857620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRCourseCount:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def mapper(self, line):\n",
    "        fields = line.strip().split(',')\n",
    "        if len(fields) > 0:\n",
    "            courseid = fields[0]\n",
    "            yield courseid, 1\n",
    "\n",
    "    def reducer(self, courseid, counts):\n",
    "        yield courseid, sum(counts)\n",
    "\n",
    "    def run(self):\n",
    "        intermediate = {}\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            next(f)  # Пропуск заголовка\n",
    "            for line in f:\n",
    "                for key, value in self.mapper(line):\n",
    "                    if key not in intermediate:\n",
    "                        intermediate[key] = []\n",
    "                    intermediate[key].append(value)\n",
    "\n",
    "        results = {}\n",
    "        for key, values in intermediate.items():\n",
    "            for result_key, result_value in self.reducer(key, values):\n",
    "                results[result_key] = result_value\n",
    "\n",
    "        # Запись результатов в файл\n",
    "        with open(self.output_file, 'w') as f:\n",
    "            for courseid, count in results.items():\n",
    "                f.write(f\"{courseid}: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92a58923-ad96-42c2-b925-0e80c7db7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'aggrigation_logs_per_week.csv'\n",
    "output_file = \"output.txt\"\n",
    "mr_job = MRCourseCount(input_file, output_file)\n",
    "mr_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44a05ce1-7204-4996-92f0-8a691dc5b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Task 1. Unix.ipynb'\t\t\t   Task_9.Onto\n",
      "'Task 2. Terminal.ipynb'\t\t   aggrigation_logs_per_week.csv\n",
      "'Task 3. Postgre.ipynb'\t\t\t   big_data_course\n",
      "'Task 4. PostGre_portal_logs.ipynb'\t   dtp201804.csv\n",
      "'Task 5. Spark.ipynb'\t\t\t   first1.ipynb\n",
      "'Task 6. Hadoop.ipynb'\t\t\t   ontology.owl\n",
      "'Task 7. Visualization (2).ipynb'\t   output.txt\n",
      "'Task 7. Visualization.ipynb.opdownload'   output_avg_s_all.txt\n",
      "'Task 8. Map_visualization.ipynb'\t   output_low_activity.txt\n",
      "'Task 9. Ontology.ipynb'\t\t   output_unique_users.txt\n",
      " Task_6.MR\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda84de-dadd-4196-9604-e8558362398e",
   "metadata": {},
   "source": [
    "# Задание 2: Подсчет среднего значения s_all_avg для каждого курса\n",
    "С использованием MapReduce вычислить среднее значение  s_all_avg по каждому курсу.\n",
    "\n",
    "Ход выполнения:\n",
    "- Map: Преобразуйте каждую строку в пару ключ-значение, где ключ — это courseid, а значение — s_all_avg.\n",
    "\n",
    "- Reduce: Вычислите среднее значение для каждого курса.\n",
    "\n",
    "- Сохраните результаты вычисления в отдельный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1f0e6ca-58b4-4fda-bfb3-3bc42bc4629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRAverageSAll:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def mapper(self, line):\n",
    "        fields = line.strip().split(',')\n",
    "        if len(fields) > 3 and fields[0] and fields[3]:\n",
    "            courseid = fields[0]\n",
    "            try:\n",
    "                s_all_avg = float(fields[3])\n",
    "                yield courseid, s_all_avg\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    def reducer(self, courseid, values):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for value in values:\n",
    "            total += value\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            yield courseid, total / count\n",
    "\n",
    "    def run(self):\n",
    "        intermediate = {}\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                for key, value in self.mapper(line):\n",
    "                    if key not in intermediate:\n",
    "                        intermediate[key] = []\n",
    "                    intermediate[key].append(value)\n",
    "\n",
    "        results = {}\n",
    "        for key, values in intermediate.items():\n",
    "            for result_key, result_value in self.reducer(key, values):\n",
    "                results[result_key] = result_value\n",
    "\n",
    "        with open(self.output_file, 'w') as f:\n",
    "            for courseid, avg in results.items():\n",
    "                f.write(f\"{courseid}: {avg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6634cefd-24d3-40ef-89f9-61fe07328b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'aggrigation_logs_per_week.csv'\n",
    "output_file = 'output_avg_s_all.txt'\n",
    "mr_job = MRAverageSAll(input_file, output_file)\n",
    "mr_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fd93e-c07e-4e87-bdc4-4d1273911292",
   "metadata": {},
   "source": [
    "# Задание 3: Подсчет количества уникальных пользователей (userid) для каждого курса\n",
    "\n",
    "Использовать MapReduce для подсчета уникальных значений.\n",
    "\n",
    "Ход выполнения:\n",
    "- Map: Преобразуйте каждую строку в пару ключ-значение, где ключ — это courseid, а значение — userid.\n",
    "\n",
    "- Reduce: Соберите уникальные userid для каждого курса и подсчитайте их количество.\n",
    "\n",
    "- Сохраните результаты вычисления в отдельный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c602d25f-5066-44e9-8864-52567d85b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRUniqueUsers:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def mapper(self, line):\n",
    "        fields = line.strip().split(',')\n",
    "        if len(fields) > 1 and fields[0] and fields[1]:\n",
    "            courseid = fields[0]\n",
    "            userid = fields[1]\n",
    "            yield courseid, userid\n",
    "\n",
    "    def reducer(self, courseid, userids):\n",
    "        unique_users = set(userids)\n",
    "        yield courseid, len(unique_users)\n",
    "\n",
    "    def run(self):\n",
    "        intermediate = {}\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                for key, value in self.mapper(line):\n",
    "                    if key not in intermediate:\n",
    "                        intermediate[key] = []\n",
    "                    intermediate[key].append(value)\n",
    "\n",
    "        results = {}\n",
    "        for key, values in intermediate.items():\n",
    "            for result_key, result_value in self.reducer(key, values):\n",
    "                results[result_key] = result_value\n",
    "\n",
    "        with open(self.output_file, 'w') as f:\n",
    "            for courseid, count in results.items():\n",
    "                f.write(f\"{courseid}: {count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65b85bc3-8921-4b63-a6f4-e6d0601a8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'aggrigation_logs_per_week.csv'\n",
    "output_file = 'output_unique_users.txt'\n",
    "mr_job = MRUniqueUsers(input_file, output_file)\n",
    "mr_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63473682-c286-4529-aeac-fc6f89bd4ade",
   "metadata": {},
   "source": [
    "# Задание 4: Запуск  MapReduce  в терминале\n",
    "\n",
    "Оформите полученный код из задания 3 в виде отдельного исполняющего файла mr_userid.py\n",
    "\n",
    "Зайдите в терминал и запустите файл mr_userid.py\n",
    "\n",
    "Например:\n",
    "```\n",
    "python untitled.py aggrigation_logs_per_week.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db6e2b-a868-44f6-aaa3-3635f7298ff6",
   "metadata": {},
   "source": [
    "Файл  mr_userid.py сохраните в отдельной папке Task_6.MR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc2fa1-f57d-44c2-85ad-6087b495892f",
   "metadata": {},
   "source": [
    "================================================\n",
    "\n",
    "\n",
    "<b>В классической реализации MapReduce (например, в Hadoop) mapper и reducer выполняются как отдельные задачи (отдельные процессы или даже отдельные узлы в кластере). </b>\n",
    "\n",
    "Преимущества разделения на mapper и reducer:\n",
    "- Масштабируемость: Mapper и reducer могут выполняться на разных узлах кластера.\n",
    "\n",
    "- Гибкость: Можно использовать разные языки программирования для mapper и reducer.\n",
    "\n",
    "- Совместимость с Hadoop: Такой подход легко адаптировать для работы с Hadoop Streaming.\n",
    "\n",
    "Исправим данное упущение. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5af6a-f5a5-4c85-afc6-43caa2fd283a",
   "metadata": {},
   "source": [
    "## Шаг 1: Разделение кода на два файла\n",
    "Файл <b>mapper.py</b>\n",
    "Этот файл будет содержать логику mapper. Он будет читать входные данные и выдавать промежуточные пары ключ-значение.\n",
    "Например:\n",
    "```\n",
    "import sys\n",
    "\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        fields = line.strip().split(',')\n",
    "        if len(fields) > 1:  # Пропуск пустых строк\n",
    "            courseid = fields[0]\n",
    "            userid = fields[1]\n",
    "            print(f\"{courseid}\\t{userid}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mapper()\n",
    "```\n",
    "\n",
    "Файл <b>reducer.py</b>\n",
    "Этот файл будет содержать логику reducer. Он будет читать промежуточные данные, группировать их по ключам и выполнять агрегацию.\n",
    "Например:\n",
    "```\n",
    "import sys\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def reducer():\n",
    "    # Чтение данных из stdin и сортировка по ключу (courseid)\n",
    "    data = [line.strip().split('\\t') for line in sys.stdin]\n",
    "    data.sort(key=itemgetter(0))\n",
    "\n",
    "    # Группировка по courseid\n",
    "    for courseid, group in groupby(data, key=itemgetter(0)):\n",
    "        userids = set(userid for _, userid in group)\n",
    "        print(f\"{courseid}\\t{len(userids)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reducer()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1daa4f5-f5b8-4c9e-8042-1708821ee132",
   "metadata": {},
   "source": [
    "## Шаг 2: Запуск mapper и reducer через конвейер\n",
    "Для запуска mapper и reducer как отдельных процессов можно использовать конвейер (pipe) в командной строке. Вот как это сделать:\n",
    "\n",
    "Запуск mapper и reducer через конвейер:\n",
    "\n",
    "```\n",
    "cat /jovyan/aggrigation_logs_per_week.csv | python3 mapper.py | sort | python3 reducer.py > output.txt\n",
    "```\n",
    "\n",
    "Здесь:\n",
    "\n",
    "- cat читает файл и передает его содержимое в mapper.py.\n",
    "\n",
    "- mapper.py обрабатывает данные и выводит промежуточные пары ключ-значение.\n",
    "\n",
    "- sort сортирует промежуточные данные по ключу (это важно для правильной работы reducer).\n",
    "\n",
    "- reducer.py обрабатывает отсортированные данные и записывает результат в output.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc520f-ec87-4848-984e-bb585e974cc1",
   "metadata": {},
   "source": [
    "Отметим, что при реализации запуска черер конвейер все данные обрабатываются на одной машине.\n",
    "\n",
    "Преимущества такого запуска:\n",
    "\n",
    "- Простота: Не требуется Hadoop или кластер.\n",
    "\n",
    "- Быстрый запуск: Подходит для локальной разработки и тестирования.\n",
    "\n",
    "- Легкость отладки: Легче отслеживать и исправлять ошибки.\n",
    "\n",
    "Недостатки:\n",
    "\n",
    "- Ограниченная масштабируемость: Все данные обрабатываются на одной машине, что не подходит для больших объемов данных.\n",
    "\n",
    "- Нет отказоустойчивости: Если процесс завершится с ошибкой, все вычисления нужно будет начать заново."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "936abc7e-0f45-4c85-bf18-32f9885d48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Task_6.MR/mr_userid.py aggrigation_logs_per_week.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc583fe-6127-4f48-ae09-44631a2bef0f",
   "metadata": {},
   "source": [
    "# Задание 5: Выявление студентов с высоким риском отчисления\n",
    "Найти студентов с низкой активностью, кто может быть в группе риска.\n",
    "- Mapper:\n",
    "\n",
    "Для каждой строки данных извлекаем userid и s_all (общая активность студента).\n",
    "\n",
    "Выдаем пары (userid, s_all).\n",
    "\n",
    "- Reducer:\n",
    "\n",
    "Для каждого userid вычисляем среднее значение s_all (средняя активность студента). Вопрос: что не так в данном подходе? Предложите, как исправить упущение (если оно имеется), реализуйте его. \n",
    "\n",
    "Сортируем студентов по средней активности и выбираем топ-10 студентов с наименьшей активностью.\n",
    "\n",
    "Два файла сохранить в папке Task_6.MR и запустить через конвейер. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e75351-0de5-47db-baba-46e95c4b0951",
   "metadata": {},
   "source": [
    "#  Запуск через Hadoop Streaming\n",
    "\n",
    "Hadoop Streaming — это утилита, которая позволяет запускать MapReduce-задачи с использованием любых исполняемых файлов (например, Python-скриптов) в качестве mapper и reducer.\n",
    "\n",
    "Она работает поверх Hadoop, что позволяет использовать распределенные вычисления на кластере.\n",
    "Преимущества запуска через Hadoop Streaming:\n",
    "\n",
    "- Масштабируемость: Hadoop распределяет данные и вычисления по множеству узлов, что позволяет обрабатывать огромные объемы данных.\n",
    "\n",
    "- Отказоустойчивость: Hadoop автоматически перезапускает задачи, если какой-то узел выходит из строя.\n",
    "\n",
    "- Интеграция с HDFS: Данные хранятся в HDFS, что обеспечивает высокую производительность и надежность.\n",
    "\n",
    "Недостатки:\n",
    "\n",
    "- Требуется настроенный Hadoop-кластер.\n",
    "\n",
    "- Более сложная настройка и запуск по сравнению с локальным подходом.\n",
    "\n",
    "Пример запуска:\n",
    "```\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
    "    -input /jovyan/aggrigation_logs_per_week.csv \\\n",
    "    -output /jovyan/output_stud \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -file ./data/big_data_course/Task_6.MR/mapper.py \\\n",
    "    -file ./data/big_data_course/Task_6.MR/reducer.py\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813da7b9-00e9-4011-a123-d0d23863a7d6",
   "metadata": {},
   "source": [
    "Обратите внимание, что файл с результатами reducer будет сохранен в директории, которую вы указали в параметре output, причем это выходная директория hdfs, что значит, что вы увидете данный файл не в локальной системе, а именно в hdfs.\n",
    "\n",
    "Найдем данный файл в hdfs:\n",
    "```\n",
    " hdfs dfs -ls -R /jovyan | grep \"output_stud\"\n",
    "```\n",
    "Пример результата поиска:\n",
    "```\n",
    "drwxr-xr-x   - jovyan hadoopusers          0 2025-03-20 14:56 /jovyan/output_stud\n",
    "-rw-r--r--   1 jovyan hadoopusers          0 2025-03-20 14:56 /jovyan/output_stud/_SUCCESS\n",
    "-rw-r--r--   1 jovyan hadoopusers       7823 2025-03-20 14:56 /jovyan/output_stud/part-00000\n",
    "```\n",
    "Внутри этой директории находятся файлы с результатами, такие как part-00000, part-00001 и т.д. _SUCCESS — пустой файл, который указывает на успешное завершение задачи. part-00000 — файл с результатами.\n",
    "\n",
    "\n",
    "Чтобы просмотреть содержимое файла с результатами, используйте команду:\n",
    "```\n",
    " hdfs dfs -cat /jovyan/output_stud/part-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "250ad836-f287-4e0c-a914-ec293ce3cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat aggrigation_logs_per_week.csv | python3 Task_6.MR/mapper.py | sort | python3 Task_6.MR/reducer.py > output_low_activity.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce25ec-e2e9-4c72-a8d7-d6dd7561f9be",
   "metadata": {},
   "source": [
    "# Задание 6: Запуск MapReduce-задачи через Hadoop Streaming \n",
    "\n",
    "Подготовьте команду для запуска MapReduce-задачи через Hadoop Streaming из задания 5.\n",
    "\n",
    "В терминале проверьте, что задача выполняется успешно.\n",
    "\n",
    "Откройте полученный файл в hdfs.\n",
    "\n",
    "Бутьте готовы продемонстриротьва запуск преподавателю. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "804094ab-586a-4239-9498-fe5e6ce41859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30 04:44:48,135 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [Task_6.MR/mapper.py, Task_6.MR/reducer.py, /tmp/hadoop-unjar1965665757870867854/] [] /tmp/streamjob3174975863444109571.jar tmpDir=null\n",
      "2025-06-30 04:44:48,785 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-06-30 04:44:48,932 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-06-30 04:44:49,119 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1751258224012_0001\n",
      "2025-06-30 04:44:50,162 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-06-30 04:44:50,213 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-06-30 04:44:50,702 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1751258224012_0001\n",
      "2025-06-30 04:44:50,703 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-06-30 04:44:50,845 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-06-30 04:44:50,845 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-06-30 04:44:51,045 INFO impl.YarnClientImpl: Submitted application application_1751258224012_0001\n",
      "2025-06-30 04:44:51,086 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:60000/proxy/application_1751258224012_0001/\n",
      "2025-06-30 04:44:51,088 INFO mapreduce.Job: Running job: job_1751258224012_0001\n",
      "2025-06-30 04:44:57,163 INFO mapreduce.Job: Job job_1751258224012_0001 running in uber mode : false\n",
      "2025-06-30 04:44:57,164 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-06-30 04:45:02,209 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-06-30 04:45:03,214 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-06-30 04:45:07,233 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-06-30 04:45:09,247 INFO mapreduce.Job: Job job_1751258224012_0001 completed successfully\n",
      "2025-06-30 04:45:09,315 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5797560\n",
      "\t\tFILE: Number of bytes written=12316490\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39034979\n",
      "\t\tHDFS: Number of bytes written=7823\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6660\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2396\n",
      "\t\tTotal time spent by all map tasks (ms)=6660\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2396\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6660\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2396\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6819840\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2453504\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=414529\n",
      "\t\tMap output records=414529\n",
      "\t\tMap output bytes=4968496\n",
      "\t\tMap output materialized bytes=5797566\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=894\n",
      "\t\tReduce shuffle bytes=5797566\n",
      "\t\tReduce input records=414529\n",
      "\t\tReduce output records=894\n",
      "\t\tSpilled Records=829058\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=373\n",
      "\t\tCPU time spent (ms)=4710\n",
      "\t\tPhysical memory (bytes) snapshot=927318016\n",
      "\t\tVirtual memory (bytes) snapshot=7626588160\n",
      "\t\tTotal committed heap usage (bytes)=854589440\n",
      "\t\tPeak Map Physical memory (bytes)=457674752\n",
      "\t\tPeak Map Virtual memory (bytes)=2539978752\n",
      "\t\tPeak Reduce Physical memory (bytes)=182726656\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2546757632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39034773\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7823\n",
      "2025-06-30 04:45:09,315 INFO streaming.StreamJob: Output directory: /jovyan/output_low_activity\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
    "    -input /jovyan/aggrigation_logs_per_week.csv \\\n",
    "    -output /jovyan/output_low_activity \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -file Task_6.MR/mapper.py \\\n",
    "    -file Task_6.MR/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be127bf4-95c3-4b5d-96b2-65c61f2cab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - jovyan hadoopusers          0 2025-06-30 04:45 /jovyan/output_low_activity\n",
      "-rw-r--r--   1 jovyan hadoopusers          0 2025-06-30 04:45 /jovyan/output_low_activity/_SUCCESS\n",
      "-rw-r--r--   1 jovyan hadoopusers       7823 2025-06-30 04:45 /jovyan/output_low_activity/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Проверка наличия выходной директории\n",
    "!hdfs dfs -ls -R /jovyan | grep \"output_low_activity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84e59900-880c-443b-8e14-859ef8dab521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71262\t20\n",
      "71269\t9\n",
      "71275\t9\n",
      "71279\t30\n",
      "71301\t9\n",
      "71302\t6\n",
      "71323\t16\n",
      "71324\t14\n",
      "71326\t11\n",
      "71335\t11\n",
      "71344\t3\n",
      "71349\t14\n",
      "71352\t21\n",
      "71354\t9\n",
      "71357\t30\n",
      "71384\t11\n",
      "71402\t8\n",
      "71409\t11\n",
      "71411\t8\n",
      "71415\t11\n",
      "71461\t14\n",
      "71467\t2\n",
      "71472\t21\n",
      "71495\t67\n",
      "71496\t28\n",
      "71508\t49\n",
      "71523\t14\n",
      "71535\t14\n",
      "71541\t64\n",
      "71545\t45\n",
      "71547\t45\n",
      "71549\t4\n",
      "71559\t12\n",
      "71571\t70\n",
      "71596\t67\n",
      "71615\t31\n",
      "71619\t46\n",
      "71632\t11\n",
      "71635\t20\n",
      "71646\t39\n",
      "71675\t64\n",
      "71697\t45\n",
      "71702\t10\n",
      "71708\t26\n",
      "71709\t46\n",
      "71710\t33\n",
      "71716\t12\n",
      "71717\t28\n",
      "71720\t16\n",
      "71730\t34\n",
      "71732\t37\n",
      "71733\t38\n",
      "71736\t30\n",
      "71740\t9\n",
      "71746\t16\n",
      "71750\t7\n",
      "71758\t7\n",
      "71776\t6\n",
      "71792\t20\n",
      "71797\t19\n",
      "71801\t6\n",
      "71806\t19\n",
      "71816\t13\n",
      "71820\t9\n",
      "71829\t12\n",
      "71831\t11\n",
      "71852\t58\n",
      "71857\t56\n",
      "71871\t13\n",
      "71880\t30\n",
      "71882\t21\n",
      "71883\t27\n",
      "71884\t35\n",
      "71885\t12\n",
      "71887\t8\n",
      "71892\t109\n",
      "71895\t21\n",
      "71902\t11\n",
      "71903\t9\n",
      "71904\t34\n",
      "71905\t11\n",
      "71906\t10\n",
      "71909\t11\n",
      "71917\t38\n",
      "71925\t41\n",
      "71930\t22\n",
      "71934\t47\n",
      "71963\t33\n",
      "71972\t23\n",
      "71976\t47\n",
      "71978\t13\n",
      "71979\t13\n",
      "71987\t19\n",
      "71995\t18\n",
      "72001\t30\n",
      "72003\t17\n",
      "72007\t13\n",
      "72023\t23\n",
      "72026\t13\n",
      "72028\t29\n",
      "72030\t23\n",
      "72032\t8\n",
      "72053\t13\n",
      "72057\t22\n",
      "72064\t5\n",
      "72073\t23\n",
      "72081\t7\n",
      "72103\t18\n",
      "72107\t26\n",
      "72119\t19\n",
      "72122\t36\n",
      "72126\t46\n",
      "72130\t15\n",
      "72141\t14\n",
      "72143\t29\n",
      "72152\t29\n",
      "72154\t41\n",
      "72155\t45\n",
      "72160\t35\n",
      "72161\t20\n",
      "72166\t27\n",
      "72167\t27\n",
      "72171\t43\n",
      "72175\t45\n",
      "72178\t64\n",
      "72179\t7\n",
      "72181\t27\n",
      "72188\t11\n",
      "72212\t8\n",
      "72232\t12\n",
      "72233\t13\n",
      "72258\t13\n",
      "72260\t25\n",
      "72263\t6\n",
      "72269\t17\n",
      "72274\t20\n",
      "72283\t12\n",
      "72285\t25\n",
      "72289\t5\n",
      "72299\t10\n",
      "72314\t22\n",
      "72321\t19\n",
      "72336\t21\n",
      "72344\t4\n",
      "72347\t19\n",
      "72350\t20\n",
      "72358\t48\n",
      "72359\t103\n",
      "72380\t119\n",
      "72387\t16\n",
      "72392\t103\n",
      "72402\t213\n",
      "72416\t118\n",
      "72423\t6\n",
      "72426\t12\n",
      "72441\t18\n",
      "72442\t43\n",
      "72447\t50\n",
      "72450\t15\n",
      "72457\t98\n",
      "72460\t73\n",
      "72462\t19\n",
      "72467\t17\n",
      "72469\t49\n",
      "72560\t19\n",
      "72572\t15\n",
      "72588\t17\n",
      "72596\t37\n",
      "72601\t9\n",
      "72611\t16\n",
      "72616\t11\n",
      "72637\t8\n",
      "72659\t16\n",
      "72666\t15\n",
      "72694\t14\n",
      "72697\t16\n",
      "72709\t15\n",
      "72711\t25\n",
      "72714\t11\n",
      "72715\t27\n",
      "72721\t13\n",
      "72722\t19\n",
      "72732\t16\n",
      "72745\t14\n",
      "72762\t19\n",
      "72790\t1\n",
      "72800\t20\n",
      "72811\t16\n",
      "72820\t16\n",
      "72831\t18\n",
      "72865\t18\n",
      "72885\t19\n",
      "72965\t6\n",
      "72970\t16\n",
      "72980\t13\n",
      "72998\t24\n",
      "73008\t24\n",
      "73021\t15\n",
      "73025\t68\n",
      "73043\t12\n",
      "73071\t15\n",
      "73079\t16\n",
      "73086\t13\n",
      "73090\t38\n",
      "73093\t24\n",
      "73099\t4\n",
      "73177\t26\n",
      "73191\t5\n",
      "73206\t14\n",
      "73210\t15\n",
      "73220\t10\n",
      "73337\t22\n",
      "73341\t13\n",
      "73354\t15\n",
      "73408\t10\n",
      "73411\t11\n",
      "73414\t11\n",
      "73544\t7\n",
      "73547\t39\n",
      "73550\t20\n",
      "73555\t7\n",
      "73560\t18\n",
      "73565\t17\n",
      "73574\t35\n",
      "73586\t32\n",
      "73731\t23\n",
      "73773\t19\n",
      "73774\t19\n",
      "73775\t18\n",
      "73823\t24\n",
      "73853\t24\n",
      "73880\t22\n",
      "73905\t19\n",
      "73934\t21\n",
      "73941\t21\n",
      "73954\t42\n",
      "73961\t22\n",
      "73981\t15\n",
      "73986\t17\n",
      "73990\t17\n",
      "74008\t13\n",
      "74238\t23\n",
      "74257\t20\n",
      "74263\t37\n",
      "74271\t17\n",
      "74277\t16\n",
      "74296\t5\n",
      "74299\t7\n",
      "74417\t21\n",
      "74435\t7\n",
      "74467\t10\n",
      "74484\t10\n",
      "74539\t3\n",
      "74546\t7\n",
      "74551\t2\n",
      "74554\t3\n",
      "74588\t3\n",
      "74591\t7\n",
      "74611\t6\n",
      "74620\t7\n",
      "74636\t4\n",
      "74639\t2\n",
      "74809\t1\n",
      "74965\t5\n",
      "74967\t9\n",
      "74969\t3\n",
      "74994\t10\n",
      "75030\t3\n",
      "75140\t13\n",
      "75142\t13\n",
      "75153\t14\n",
      "75244\t9\n",
      "75633\t25\n",
      "75634\t23\n",
      "75638\t21\n",
      "75643\t22\n",
      "75649\t20\n",
      "75650\t21\n",
      "75653\t21\n",
      "75656\t22\n",
      "75659\t7\n",
      "75666\t19\n",
      "75669\t7\n",
      "75670\t6\n",
      "75684\t5\n",
      "75690\t10\n",
      "75701\t19\n",
      "75703\t1\n",
      "75705\t4\n",
      "75714\t13\n",
      "75715\t8\n",
      "75716\t12\n",
      "75717\t11\n",
      "75732\t3\n",
      "75735\t5\n",
      "75736\t13\n",
      "75738\t30\n",
      "75755\t25\n",
      "75756\t11\n",
      "75765\t12\n",
      "75782\t20\n",
      "75783\t1\n",
      "75790\t14\n",
      "75791\t34\n",
      "75793\t10\n",
      "75801\t26\n",
      "75809\t27\n",
      "75810\t131\n",
      "75819\t11\n",
      "75823\t10\n",
      "75824\t12\n",
      "75827\t6\n",
      "75830\t6\n",
      "75833\t175\n",
      "75834\t63\n",
      "75839\t99\n",
      "75840\t13\n",
      "75849\t133\n",
      "75851\t12\n",
      "75861\t14\n",
      "75866\t4\n",
      "75868\t14\n",
      "75870\t4\n",
      "75891\t5\n",
      "75892\t13\n",
      "75903\t8\n",
      "75921\t16\n",
      "75922\t8\n",
      "75943\t26\n",
      "75956\t10\n",
      "75960\t10\n",
      "75963\t27\n",
      "75964\t9\n",
      "75965\t8\n",
      "75967\t47\n",
      "75969\t10\n",
      "75970\t41\n",
      "75974\t15\n",
      "75984\t21\n",
      "75993\t17\n",
      "76016\t21\n",
      "76017\t21\n",
      "76035\t14\n",
      "76071\t1\n",
      "76081\t1\n",
      "76088\t1\n",
      "76096\t2\n",
      "76106\t4\n",
      "76107\t7\n",
      "76116\t1\n",
      "76161\t1\n",
      "76162\t4\n",
      "76170\t2\n",
      "76171\t6\n",
      "76180\t1\n",
      "76181\t2\n",
      "76226\t6\n",
      "76227\t1\n",
      "76238\t1\n",
      "76293\t35\n",
      "76308\t16\n",
      "76325\t26\n",
      "76372\t47\n",
      "76373\t22\n",
      "76375\t15\n",
      "76411\t44\n",
      "76414\t48\n",
      "76419\t41\n",
      "76452\t11\n",
      "76544\t11\n",
      "76549\t14\n",
      "76555\t13\n",
      "76566\t24\n",
      "76567\t22\n",
      "76568\t24\n",
      "76608\t1\n",
      "76613\t1\n",
      "76765\t2\n",
      "78057\t243\n",
      "78110\t19\n",
      "78120\t3\n",
      "78124\t13\n",
      "78163\t6\n",
      "78169\t9\n",
      "78186\t17\n",
      "78198\t14\n",
      "78201\t6\n",
      "78232\t9\n",
      "78240\t18\n",
      "78241\t27\n",
      "78263\t22\n",
      "78272\t26\n",
      "78285\t24\n",
      "78286\t54\n",
      "78287\t28\n",
      "78288\t52\n",
      "78312\t22\n",
      "78328\t23\n",
      "78331\t27\n",
      "78349\t7\n",
      "78352\t16\n",
      "78382\t10\n",
      "78384\t8\n",
      "78443\t21\n",
      "78451\t14\n",
      "78462\t25\n",
      "78471\t36\n",
      "78495\t4\n",
      "78498\t35\n",
      "78519\t11\n",
      "78521\t11\n",
      "78525\t11\n",
      "78537\t23\n",
      "78541\t23\n",
      "78544\t19\n",
      "78572\t30\n",
      "78582\t13\n",
      "78603\t12\n",
      "78615\t6\n",
      "78643\t11\n",
      "78653\t16\n",
      "78654\t19\n",
      "78660\t15\n",
      "78673\t18\n",
      "78680\t11\n",
      "78683\t10\n",
      "78689\t16\n",
      "78705\t20\n",
      "78733\t12\n",
      "78773\t19\n",
      "78843\t36\n",
      "78886\t6\n",
      "78892\t26\n",
      "78893\t55\n",
      "78894\t20\n",
      "78978\t1\n",
      "78988\t12\n",
      "79007\t16\n",
      "79035\t23\n",
      "79054\t16\n",
      "79055\t21\n",
      "79058\t23\n",
      "79064\t17\n",
      "79068\t18\n",
      "79097\t21\n",
      "79098\t20\n",
      "79104\t19\n",
      "79114\t20\n",
      "79130\t11\n",
      "79291\t12\n",
      "79293\t12\n",
      "79382\t7\n",
      "79392\t28\n",
      "79397\t30\n",
      "79403\t4\n",
      "79407\t5\n",
      "79409\t7\n",
      "79412\t4\n",
      "79418\t10\n",
      "79426\t255\n",
      "79430\t15\n",
      "79461\t21\n",
      "79464\t22\n",
      "79466\t19\n",
      "79558\t2\n",
      "79567\t2\n",
      "79835\t7\n",
      "79893\t17\n",
      "79894\t1\n",
      "79901\t22\n",
      "79912\t31\n",
      "79913\t23\n",
      "79928\t16\n",
      "81752\t13\n",
      "81754\t13\n",
      "81762\t8\n",
      "81773\t14\n",
      "81776\t18\n",
      "81783\t10\n",
      "81786\t10\n",
      "81789\t10\n",
      "81795\t9\n",
      "81832\t6\n",
      "81847\t73\n",
      "81848\t22\n",
      "81860\t40\n",
      "81868\t13\n",
      "81870\t13\n",
      "81871\t7\n",
      "81888\t14\n",
      "81912\t10\n",
      "81918\t16\n",
      "81938\t14\n",
      "81940\t20\n",
      "81943\t16\n",
      "81946\t25\n",
      "81961\t2\n",
      "81974\t19\n",
      "81976\t15\n",
      "81985\t16\n",
      "81995\t14\n",
      "82000\t15\n",
      "82007\t17\n",
      "82035\t18\n",
      "82058\t38\n",
      "82059\t24\n",
      "82089\t9\n",
      "82135\t35\n",
      "82182\t29\n",
      "82215\t14\n",
      "82226\t19\n",
      "82230\t16\n",
      "82234\t19\n",
      "82260\t23\n",
      "82273\t20\n",
      "82281\t16\n",
      "82295\t27\n",
      "82298\t28\n",
      "82321\t7\n",
      "82335\t8\n",
      "82345\t9\n",
      "82346\t9\n",
      "82357\t8\n",
      "82389\t11\n",
      "82399\t11\n",
      "82409\t4\n",
      "82464\t2\n",
      "82521\t8\n",
      "82522\t8\n",
      "82531\t44\n",
      "82532\t25\n",
      "82536\t10\n",
      "82552\t24\n",
      "82558\t26\n",
      "82559\t1\n",
      "82565\t17\n",
      "82584\t26\n",
      "82595\t18\n",
      "82622\t1\n",
      "82647\t3\n",
      "82668\t23\n",
      "82672\t20\n",
      "82698\t6\n",
      "82717\t31\n",
      "82719\t33\n",
      "82754\t17\n",
      "82755\t18\n",
      "82779\t8\n",
      "83191\t17\n",
      "83193\t18\n",
      "83198\t23\n",
      "83209\t1\n",
      "83210\t1\n",
      "83257\t5\n",
      "83290\t12\n",
      "83674\t21\n",
      "83683\t8\n",
      "83685\t10\n",
      "83690\t49\n",
      "83691\t10\n",
      "83703\t9\n",
      "83736\t56\n",
      "83751\t30\n",
      "83753\t1\n",
      "83768\t35\n",
      "83784\t10\n",
      "83799\t7\n",
      "83811\t19\n",
      "83843\t20\n",
      "83844\t13\n",
      "83853\t48\n",
      "83866\t28\n",
      "83894\t9\n",
      "83930\t19\n",
      "83960\t8\n",
      "83962\t16\n",
      "83964\t15\n",
      "83965\t5\n",
      "83969\t3\n",
      "83974\t4\n",
      "83975\t16\n",
      "83979\t4\n",
      "83989\t22\n",
      "84002\t3\n",
      "84030\t28\n",
      "84035\t10\n",
      "84067\t28\n",
      "84080\t22\n",
      "84085\t23\n",
      "84092\t22\n",
      "84103\t25\n",
      "84110\t24\n",
      "84113\t24\n",
      "84124\t21\n",
      "84125\t22\n",
      "84130\t21\n",
      "84139\t18\n",
      "84140\t19\n",
      "84143\t17\n",
      "84165\t23\n",
      "84173\t20\n",
      "84176\t23\n",
      "84181\t20\n",
      "84215\t19\n",
      "84219\t20\n",
      "84222\t20\n",
      "84223\t20\n",
      "84224\t20\n",
      "84233\t22\n",
      "84236\t22\n",
      "84240\t21\n",
      "84243\t22\n",
      "84244\t22\n",
      "84263\t20\n",
      "84265\t21\n",
      "84266\t20\n",
      "84275\t21\n",
      "84279\t22\n",
      "84282\t50\n",
      "84299\t27\n",
      "84310\t23\n",
      "84316\t23\n",
      "84318\t24\n",
      "84319\t24\n",
      "84321\t23\n",
      "84328\t24\n",
      "84332\t45\n",
      "84335\t57\n",
      "84341\t23\n",
      "84352\t16\n",
      "84369\t8\n",
      "84370\t6\n",
      "84379\t8\n",
      "84381\t8\n",
      "84384\t8\n",
      "84403\t17\n",
      "84405\t18\n",
      "84463\t9\n",
      "84470\t6\n",
      "84567\t10\n",
      "84624\t11\n",
      "84628\t11\n",
      "84655\t20\n",
      "84656\t9\n",
      "84669\t9\n",
      "84810\t15\n",
      "84816\t14\n",
      "84820\t14\n",
      "84825\t8\n",
      "84834\t81\n",
      "84837\t9\n",
      "84845\t20\n",
      "84846\t27\n",
      "84850\t26\n",
      "84852\t19\n",
      "84853\t22\n",
      "84854\t12\n",
      "84855\t18\n",
      "84863\t15\n",
      "84866\t15\n",
      "84879\t38\n",
      "84958\t18\n",
      "84967\t20\n",
      "84968\t17\n",
      "84979\t29\n",
      "84980\t31\n",
      "84984\t15\n",
      "84989\t13\n",
      "84994\t25\n",
      "85001\t16\n",
      "85007\t20\n",
      "85025\t1\n",
      "85072\t5\n",
      "85074\t17\n",
      "85075\t18\n",
      "85080\t5\n",
      "85128\t9\n",
      "86053\t21\n",
      "86066\t4\n",
      "86219\t2\n",
      "86233\t1\n",
      "86639\t5\n",
      "86656\t15\n",
      "86660\t16\n",
      "86663\t13\n",
      "86690\t14\n",
      "86712\t18\n",
      "86721\t31\n",
      "86730\t9\n",
      "86743\t2\n",
      "86802\t46\n",
      "86806\t16\n",
      "86810\t19\n",
      "86820\t23\n",
      "86832\t35\n",
      "86834\t52\n",
      "86861\t11\n",
      "86897\t15\n",
      "86914\t14\n",
      "86940\t11\n",
      "86973\t25\n",
      "86984\t32\n",
      "86985\t32\n",
      "86997\t19\n",
      "87008\t16\n",
      "87014\t15\n",
      "87034\t14\n",
      "87039\t21\n",
      "87058\t18\n",
      "87063\t18\n",
      "87071\t10\n",
      "87077\t20\n",
      "87081\t20\n",
      "87084\t16\n",
      "87087\t33\n",
      "87097\t15\n",
      "87109\t14\n",
      "87124\t20\n",
      "87379\t8\n",
      "87380\t8\n",
      "87385\t8\n",
      "87391\t3\n",
      "87392\t28\n",
      "87393\t12\n",
      "87396\t125\n",
      "87397\t20\n",
      "87400\t19\n",
      "87401\t21\n",
      "87495\t5\n",
      "87932\t14\n",
      "87935\t13\n",
      "87944\t18\n",
      "87972\t7\n",
      "87973\t5\n",
      "87975\t14\n",
      "88285\t16\n",
      "88290\t30\n",
      "88314\t24\n",
      "88316\t16\n",
      "88320\t15\n",
      "88321\t10\n",
      "88329\t27\n",
      "88333\t25\n",
      "88335\t27\n",
      "88345\t11\n",
      "88347\t15\n",
      "88349\t62\n",
      "88387\t9\n",
      "88389\t13\n",
      "88392\t12\n",
      "88411\t19\n",
      "88415\t46\n",
      "88422\t8\n",
      "88429\t3\n",
      "88430\t2\n",
      "88446\t16\n",
      "88458\t17\n",
      "88463\t19\n",
      "88474\t14\n",
      "88477\t1\n",
      "88489\t46\n",
      "88502\t17\n",
      "88504\t17\n",
      "88513\t8\n",
      "88523\t27\n",
      "88532\t9\n",
      "88545\t30\n",
      "88546\t36\n",
      "88552\t28\n",
      "88556\t23\n",
      "88569\t15\n",
      "88575\t16\n",
      "88594\t14\n",
      "88598\t14\n",
      "88601\t13\n",
      "88603\t11\n",
      "88608\t16\n",
      "88610\t16\n",
      "88618\t16\n",
      "88619\t16\n",
      "88620\t16\n",
      "88624\t21\n",
      "88627\t21\n",
      "88631\t21\n",
      "88653\t14\n",
      "88659\t15\n",
      "88661\t20\n",
      "88665\t21\n",
      "88667\t20\n",
      "88668\t1\n",
      "88692\t3\n",
      "88713\t37\n",
      "88721\t13\n",
      "88732\t7\n",
      "88733\t8\n",
      "88734\t6\n",
      "88741\t4\n",
      "88748\t4\n",
      "88831\t16\n",
      "88835\t16\n",
      "88838\t16\n",
      "88843\t15\n",
      "88849\t15\n",
      "88853\t15\n",
      "88856\t14\n",
      "88857\t17\n",
      "88860\t13\n",
      "88870\t27\n",
      "88873\t29\n",
      "88876\t29\n",
      "88878\t29\n",
      "88879\t25\n",
      "88887\t16\n",
      "88888\t17\n",
      "88892\t15\n",
      "88912\t18\n",
      "88921\t11\n",
      "88933\t6\n",
      "88942\t9\n",
      "88944\t8\n",
      "88945\t10\n",
      "88960\t7\n",
      "88963\t7\n",
      "88974\t6\n",
      "88979\t4\n",
      "88990\t10\n",
      "88991\t10\n",
      "89012\t5\n",
      "89016\t5\n",
      "89017\t6\n",
      "89024\t7\n",
      "89029\t8\n",
      "89034\t8\n",
      "89041\t8\n",
      "89044\t8\n",
      "89046\t8\n",
      "89062\t7\n",
      "89068\t7\n",
      "89069\t6\n",
      "89072\t10\n",
      "89073\t7\n",
      "89079\t7\n",
      "89084\t8\n",
      "89096\t7\n",
      "89099\t7\n",
      "89105\t7\n",
      "89118\t2\n",
      "89121\t4\n",
      "89124\t10\n",
      "89139\t9\n",
      "89140\t9\n",
      "89142\t10\n",
      "89148\t7\n",
      "89150\t7\n",
      "89154\t7\n",
      "89161\t9\n",
      "89165\t11\n",
      "89168\t11\n",
      "89172\t10\n",
      "89185\t9\n",
      "89192\t9\n",
      "89193\t9\n",
      "89200\t5\n",
      "89201\t5\n",
      "89218\t7\n",
      "89234\t5\n",
      "89262\t8\n",
      "89266\t8\n",
      "89281\t11\n",
      "89286\t11\n",
      "89311\t13\n",
      "89313\t13\n",
      "89314\t13\n",
      "89319\t12\n",
      "89328\t10\n",
      "89351\t43\n",
      "89353\t25\n",
      "89355\t17\n",
      "89379\t10\n",
      "89387\t10\n",
      "89388\t10\n",
      "89393\t11\n",
      "89394\t1\n",
      "89400\t1\n",
      "89547\t8\n",
      "89778\t15\n",
      "89785\t11\n",
      "89789\t13\n",
      "89795\t1\n",
      "89797\t6\n",
      "89815\t1\n",
      "89824\t10\n",
      "89948\t1\n",
      "courseid\t1\n"
     ]
    }
   ],
   "source": [
    "# Просмотр содержимого результата\n",
    "!hdfs dfs -cat /jovyan/output_low_activity/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff73efb-fc0a-44b3-b0df-cf9b34405d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
